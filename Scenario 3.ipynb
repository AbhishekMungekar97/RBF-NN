{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3 : Using K-means clustering method to select centers from the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Square Error of the training data is:  4.037634410752571 %\n",
      "The Mean Square Error of the test data is:  9.30517700858365 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Training the Radial basis function\n",
    "class Rad_bas_func:\n",
    "    \n",
    "    def __init__(self, input_dim, num_of_Centers, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_of_Centers = num_of_Centers\n",
    "        self.centers = None \n",
    "        self.W = None\n",
    "         \n",
    "    def Gauss_kernal_func(self, c, d):\n",
    "        # utilizing appropriate spread parameter for the model\n",
    "        sigma = 3.5\n",
    "        return exp((- norm(c-d)**2)/2*sigma**2)\n",
    "     \n",
    "    def Activation_func(self, X):\n",
    "        # calculate activations of RBFs\n",
    "        G = zeros((X.shape[0], self.num_of_Centers), float)\n",
    "        for ci, c in enumerate(self.centers):\n",
    "            for xi, x in enumerate(X):\n",
    "                G[xi,ci] = self.Gauss_kernal_func(c, x)\n",
    "        return G\n",
    "    \n",
    "    def K_means(self, X):\n",
    "        # randomly selecting input data as the centers\n",
    "        km= KMeans(n_clusters= 150 , max_iter= 1)\n",
    "        km.fit(X)\n",
    "        centers = km.cluster_centers_\n",
    "        return centers\n",
    "     \n",
    "    def train(self, X, Y): \n",
    "        # randomly selecting input data as the centers\n",
    "        self.centers = self.K_means(X)\n",
    "        # calculating the activation function based on the dataset provided\n",
    "        G = self.Activation_func(X)\n",
    "        # Since the G matrix may be ill-conditioned, therefore it is crucial to take its psuedoinverse\n",
    "        self.W = dot(pinv(G), Y)\n",
    "         \n",
    "    def prediction(self, X):\n",
    "        # predicting all the labels, in order to tabulate error\n",
    "        G = self.Activation_func(X)\n",
    "        Y = dot(G, self.W)\n",
    "        return Y\n",
    "    def mean_square_error_train(self,y_true,y_pred):\n",
    "        dif_list=[]\n",
    "        for i in range(352):\n",
    "            #finding the difference between observed and predicted value\n",
    "            difference = y_true[i] - y_pred[i] \n",
    "            #taking square of the differene\n",
    "            squared_difference = difference**2   \n",
    "            dif_list.append(squared_difference)\n",
    "        #dividing summation by total values to obtain average    \n",
    "        MSE = sum(dif_list)/352 \n",
    "        print (\"The Mean Square Error of the training data is: \" , MSE*100,\"%\")\n",
    "        \n",
    "    def mean_square_error_test(self,y_true,y_pred):\n",
    "        dif_list=[]\n",
    "        for i in range(89):\n",
    "            #finding the difference between observed and predicted value\n",
    "            difference = y_true[i] - y_pred[i] \n",
    "            #taking square of the differene\n",
    "            squared_difference = difference**2   \n",
    "            dif_list.append(squared_difference)\n",
    "        #dividing summation by total values to obtain average    \n",
    "        MSE = sum(dif_list)/89\n",
    "        print (\"The Mean Square Error of the test data is: \" , MSE*100,\"%\")\n",
    " \n",
    " \n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    x= training_data\n",
    "    y = training_labels\n",
    "    m = test_data\n",
    "    k = test_labels\n",
    "    #Inputing necassary parameters for the radial basis fucntion model\n",
    "    rbf = Rad_bas_func(2, 150, 1)\n",
    "    rbf.train(x, y)\n",
    "    n = rbf.prediction(x)\n",
    "    rbf.mean_square_error_train(n,y)\n",
    "    z= rbf.prediction(m)\n",
    "    rbf.mean_square_error_test(k,z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations :-\n",
    "### - From Scenario 2 and 3 it was observed that, for a spread parameter of 0.7, the K-means clustering method comparitively did a much better job at center allotment than the random method of center allotment.\n",
    "### - The training error was the least in case of K-means method with 150 nodes, in comparision to the random allotment of nodes. Moreover this training error is much better than the one deduced in part 1 where we utilized 441 nodes.\n",
    "### - In terms of testing error, the K-means again slightly outperforms the random method and the best case scenario of part 1.\n",
    "### - Therefore, it can be concluded the k-means method of clustering does a much better job at optimizing the performance of the model in comparison to the rest. Hence, it is one of the most widely used method alongside RBF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
